# import libraries
import torch  # machine learning
import os.path  # detects if file exists
import glob  # collects all files in path
import numpy as np #for 2d array
from transformers import BertTokenizer, BertModel  # bert model
from tabulate import tabulate # print 2d array

# input query
query = []
query.append(input("Search: "))

# create class for dataset
class my_dataset(torch.utils.data.Dataset):
    def __init__(self, filename):

        # clean text files
        def clean_text(filename):
            import re  # substitution
            import nltk  # natural language tool kit

            with open(filename, 'r') as f:
                text = f.read()
            sentence_list = nltk.sent_tokenize(text)  # make sentences

            for i in range(0, len(sentence_list)):
                sentence_list[i] = sentence_list[i].strip()  # get rid of unncessary spaces
                # get rid of unnecessary chars
                sentence_list[i] = re.sub("@\S+", "", sentence_list[i])
                sentence_list[i] = re.sub("#", "", sentence_list[i])
                sentence_list[i] = re.sub("\n", " ", sentence_list[i])
                sentence_list[i] = re.sub("-", "", sentence_list[i])
                sentence_list[i] = re.sub("[\(\[].*?[\)\]]", "", sentence_list[i])
                
                if sentence_list[i].find(' ') == -1:  # get rid of one word sentences/symbols
                    sentence_list[i] = None
            sentence_list = [i for i in sentence_list if i]
            
            return sentence_list

        self.sentences = clean_text(filename)

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        x = self.sentences[idx]
        return x

# create bert model
class BERT_Model(torch.nn.Module):
    def __init__(self, pre_trained, tokenizer):
        super(BERT_Model, self).__init__()
        self.bert = pre_trained
        self.tokenizer = tokenizer
        # bert output
        self.output_dim = self.bert.config.hidden_size

    def forward(self, x):
        model_output = self.bert(**x).last_hidden_state.detach()
        model_output = torch.mean(model_output, dim=1)
        return model_output

    def encode(self, sentences):
        tokenizes_sentences = self.tokenizer(sentences)
        tokenizes_sentences = tokenizes_sentences.to(device)
        return self.forward(tokenizes_sentences)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

def BERT_tokenizer(batch):
    return bert_tokenizer(batch, return_tensors="pt", padding=True, truncation=True)

# bert model
my_encoder = BERT_Model(bert_model, BERT_tokenizer)

# create embedding for files and store in pt file
txt_list = ["/home/marie/Downloads/sample_txt_pgs/Ahuja_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Titone2002_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Gill_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Ramsey2003_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Sommer2006_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Isler2017_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Johnson2012_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Morris2011_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Rice2003_pg1.txt",
            "/home/marie/Downloads/sample_txt_pgs/Lim2019_pg1.txt"]

for file in txt_list:
    name = file.split('/')
    name = name[-1].replace(".txt", ".pt")
    pt_file_name = "/home/marie/Downloads/test_pt/" + name

    if os.path.exists(pt_file_name):
        continue

    dataset = my_dataset(file)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True, num_workers=1,
                                              drop_last=False, collate_fn=BERT_tokenizer)  # convert text into matrix

    file_embedding = []

    for x in data_loader:
        file_embedding.append(my_encoder(x))

    torch.save(file_embedding, pt_file_name)  # save in pt file

# create embedding for query
data_loader = torch.utils.data.DataLoader(query, batch_size=1, shuffle=False, pin_memory=True, num_workers=1,
                                          drop_last=False, collate_fn=BERT_tokenizer)

query_embedding = []

for x in data_loader:
    query_embedding.append(my_encoder(x))

# cosine similarity: want similarity 1
pt_list = glob.glob("/home/marie/Downloads/test_pt/*.pt")
similarity_list = []
result_list = []
cos = torch.nn.CosineSimilarity(dim=1)

for pt_file in pt_list:
    pt_load = torch.load(pt_file)
    similarity = 0
    sentence_num = 0
    for pt_sentence in pt_load:
        sentence_num = sentence_num + 1
        similarity = cos(pt_sentence, query_embedding[0])

        name = pt_file.split('/')
        name = name[-1] + f"_sentence{sentence_num}"
        name = f"{similarity}: " + name

        similarity_list.append(name)
    similarity_list.sort(reverse=True)

# return top 5 results: result number, file, page, sentence
num = 5
for i in range(0, num):
    file_to_return = similarity_list[i]
    file_to_return = file_to_return.split()
    file_to_return = file_to_return[-1].replace(".pt", ".txt")
    result_list.append(file_to_return)

print("\nResults:\n")

display = np.empty((0, 4), str) # 2d array using numpy
order = 0

for txt in result_list:
    order = order + 1
    result = txt.split("_")
    result[0] = result[0] + ".txt" # result[0] = file name
    result[1] = result[1].replace(".txt", "")
    result[1] = result[1].replace("pg", "Page ") # result[1] = page number
    result[-1] = result[-1].replace("sentence", "") # result[-1] = sentence number
    num = int(result[-1])

    file = txt.split("_sentence")
    file = file[0]
    dataset = my_dataset(f"/home/marie/Downloads/sample_txt_pgs/{file}")

    display = np.append(display, np.array([[f"{order}", result[0], result[1], f"'{dataset[num - 1]}'"]]), axis=0) # append results

print(tabulate(display, headers=["Result", "File", "Page", "Text"])) # print with headers
